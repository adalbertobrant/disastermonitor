# ATEN√á√ÉO O PROJETO AINDA N√ÉO EST√Å PRONTO E POSSUI ERROS

# üåé Intelligent Disaster Monitor (Google Gemini Edition)

### üîç Intelig√™ncia Artificial multiagente para monitoramento em tempo real de riscos naturais e impactos econ√¥micos, utilizando Google Gemini.

Este projeto utiliza agentes especializados (climatologista, sism√≥logo, economista, especialista solar, analista de seguros) que se retroalimentam por meio da API Google Generative AI (Gemini) para interpretar dados em tempo real de desastres naturais e antecipar impactos econ√¥micos. Baseado em web scraping (Selenium headless para JS, Requests para est√°tico), an√°lise ass√≠ncrona e integra√ß√£o com APIs como FRED (St. Louis Fed), USGS, NOAA, e alertas por Telegram.

---

## üß† Componentes principais

- üåê **Web Scraping H√≠brido**: Selenium (headless Chrome) para sites com JavaScript e `requests` para sites est√°ticos.
- üß¨ **Agentes com Intelig√™ncia Artificial via Google Generative AI (Gemini API)**.
- üìä **Banco de dados SQLite3** para armazenamento hist√≥rico de cen√°rios e an√°lises.
- üîî **Alertas autom√°ticos via Telegram** para novos cen√°rios e recomenda√ß√µes.
- üîÑ **Execu√ß√£o em Ciclos**: O sistema opera em ciclos, coletando dados, analisando e gerando insights periodicamente.
- üß± **Frontend Interativo com Streamlit**: Visualiza√ß√£o de dados, cen√°rios e simula√ß√£o de estrat√©gias de trading.
- üì¶ **Container Docker Leve e Port√°til**: Para f√°cil deploy e execu√ß√£o consistente.
- ‚òÅÔ∏è **Compat√≠vel com Google Colab (com ajustes), desktop ou nuvem**.

---

## üì¶ Tecnologias e Bibliotecas

| Componente             | Vers√£o/Tecnologia        |
|------------------------|--------------------------|
| Python                 | >= 3.10                  |
| Google Generative AI   | `google-generativeai`    |
| Web Scraping           | `selenium`, `beautifulsoup4`, `requests` |
| HTTP Client (Async)    | `aiohttp` (opcional, para futuras otimiza√ß√µes) |
| Environment Variables  | `python-dotenv`          |
| Database               | `sqlite3` (nativo)       |
| Logging                | `logging` (nativo)       |
| Frontend               | `streamlit`, `pandas`    |
| Telegram Integration   | `requests`               |
| Containerization       | `Docker`                 |

---

## üìÇ Estrutura do Projeto


---

## üöÄ Como executar

### üîß Requisitos
- Python >= 3.10
- Google Chrome e ChromeDriver (se n√£o usar Docker, e ChromeDriver deve estar no PATH ou especificado)
- API Keys (Google AI, FRED, Telegram)

### üìù Configura√ß√£o Inicial
1.  Clone o reposit√≥rio:
    ```bash
    git clone <repository_url>
    cd intelligent_disaster_monitor
    ```
2.  Crie um ambiente virtual (recomendado):
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    # venv\Scripts\activate    # Windows
    ```
3.  Instale as depend√™ncias:
    ```bash
    pip install -r requirements.txt
    ```
4.  Crie o arquivo `.env` na raiz do projeto (copie de `.env.example` se existir, ou crie manualmente) e preencha suas chaves de API:
    ```ini
    # .env
    GOOGLE_API_KEY="YOUR_GOOGLE_API_KEY"
    FRED_API_KEY="YOUR_FRED_API_KEY"
    TELEGRAM_BOT_TOKEN="YOUR_TELEGRAM_BOT_TOKEN"
    TELEGRAM_CHAT_ID="YOUR_TELEGRAM_CHAT_ID"
    ```

### üèÉ Executando o Sistema de Agentes (Backend)

Diretamente com Python:
```bash
python -m src.agent_system

streamlit run frontend/frontend_interface.py

docker build -t intelligent-disaster-monitor .

Isso iniciar√° o ciclo de monitoramento cont√≠nuo.
üñ•Ô∏è Executando o Frontend (Streamlit)
Em um novo terminal (com o ambiente virtual ativado):
streamlit run frontend/frontend_interface.py
Use code with caution.
Bash
Acesse o dashboard no navegador, geralmente em http://localhost:8501.
üê≥ Com Docker
Construa a imagem Docker:
Certifique-se de que o Docker Desktop (ou Docker Engine no Linux) est√° em execu√ß√£o.
No diret√≥rio raiz do projeto (onde o Dockerfile est√° localizado):
docker build -t intelligent-disaster-monitor .
Use code with caution.
Bash
Execute o container Docker (Sistema de Agentes):
Voc√™ precisar√° passar suas vari√°veis de ambiente para o container. A forma mais f√°cil √© usando um arquivo .env.
docker run -d --name disaster-monitor-agent --env-file .env intelligent-disaster-monitor
Use code with caution.
Bash
-d: Executa em modo detached (background).
--name disaster-monitor-agent: Nomeia o container.
--env-file .env: Carrega vari√°veis de ambiente do arquivo .env.
intelligent-disaster-monitor: Nome da imagem que voc√™ construiu.
Para ver os logs do container do agente:
docker logs -f disaster-monitor-agent
Use code with caution.
Bash
Execute o container Docker (Frontend - Opcional, se quiser rodar o frontend em Docker tamb√©m):
Se voc√™ quiser rodar o frontend em Docker, voc√™ precisaria de um CMD diferente no Dockerfile ou uma imagem separada.
Para este exemplo, rodar o frontend localmente (como descrito acima) enquanto o backend roda em Docker √© mais simples se eles compartilham o arquivo de banco de dados via volume.
Se o banco de dados data/disaster_monitor.db √© criado pelo container do agente, o frontend (rodando localmente ou em outro container) precisa acessar esse arquivo.
Para compartilhar o data diret√≥rio com o container do agente:
docker run -d --name disaster-monitor-agent \
  --env-file .env \
  -v "$(pwd)/data:/app/data" \
  intelligent-disaster-monitor
Use code with caution.
Bash
Assim, o data/disaster_monitor.db criado pelo container estar√° dispon√≠vel no seu host no diret√≥rio data/, e o Streamlit (rodando localmente) poder√° acess√°-lo.
üõ†Ô∏è Considera√ß√µes sobre Scraping Avan√ßado (Ex: Toro Investimentos)
A infraestrutura de scraping atual com Selenium √© um ponto de partida. Para plataformas financeiras complexas como a Toro Investimentos, seriam necess√°rias t√©cnicas mais avan√ßadas:
Login e Gerenciamento de Sess√£o:
Automatizar o login de forma segura (evitar hardcoding de credenciais).
Manter e reutilizar cookies de sess√£o.
Navega√ß√£o Din√¢mica e Waits Expl√≠citos:
Uso intensivo de WebDriverWait para esperar por elementos espec√≠ficos (gr√°ficos, tabelas de cota√ß√µes, feeds de not√≠cias) antes de tentar interagir ou extrair dados.
Identificar XHR/Fetch requests que carregam dados dinamicamente e potencialmente intercept√°-los ou simular sua l√≥gica.
Tratamento de CAPTCHAs:
Muito desafiador. Pode envolver servi√ßos de terceiros para resolu√ß√£o de CAPTCHAs (ex: Anti-CAPTCHA, 2Captcha) ou t√©cnicas para minimizar sua apari√ß√£o.
Estrutura do Site Alvo:
An√°lise detalhada da estrutura HTML/CSS/JS do site para identificar seletores robustos.
Adapta√ß√£o a mudan√ßas frequentes na interface do usu√°rio do site.
Evas√£o de Detec√ß√£o:
Rota√ß√£o de User-Agents.
Uso de proxies (residenciais ou de datacenters) para evitar bloqueios de IP.
Simula√ß√£o de comportamento humano (movimentos de mouse, delays aleat√≥rios).
Robustez e Tratamento de Erros Espec√≠ficos:
Mecanismos de retry mais sofisticados para falhas de rede ou elementos n√£o encontrados.
Logging detalhado de cada etapa do scraping para depura√ß√£o.
A implementa√ß√£o dessas t√©cnicas requer um esfor√ßo consider√°vel e customiza√ß√£o para cada site alvo. O framework atual prov√™ a base para integrar tais m√≥dulos de scraping avan√ßado.
---

**Final Checks and How to Run:**

1.  **Install Dependencies:** `pip install -r requirements.txt`
2.  **Set up `.env`:** Create and populate your `.env` file in the project root.
3.  **Run the Agent System (Backend):**
    ```bash
    python -m src.agent_system
    ```
    This will start logging to console, create/use `data/disaster_monitor.db`, and send Telegram alerts if configured.
4.  **Run the Frontend:**
    In a *new terminal*:
    ```bash
    streamlit run frontend/frontend_interface.py
    ```
    Open your browser to the URL Streamlit provides (usually `http://localhost:8501`).

**Docker Execution:**
1. Build: `docker build -t intelligent-disaster-monitor .`
2. Run Agent System: `docker run -d --name disaster-monitor-agent --env-file .env -v "$(pwd)/data:/app/data" intelligent-disaster-monitor`
   The `-v "$(pwd)/data:/app/data"` mounts the local `data` directory into the container's `/app/data` directory. This way, the SQLite database created by the agent system inside the Docker container will be persistent on your host machine in the `data` folder. The Streamlit app (run locally) can then access this same database file.

This comprehensive refactor provides a much more robust, maintainable, and extensible system using Google Gemini for its AI capabilities. The scraping part is still foundational but allows for plugging in more advanced techniques as needed.
